{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os\n","import sys"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import cv2\n","import numpy as np\n","import torch\n","import albumentations\n","import matplotlib.pyplot as plt\n","import glob\n","import math\n","from PIL import Image as Image\n","import torchvision\n","from torchvision import transforms\n","import sklearn.metrics\n","from sklearn.model_selection import StratifiedKFold\n","import torch.nn as nn\n","import torch.nn.functional as FT\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from tqdm import tqdm as tqdm\n","from albumentations.core.transforms_interface import ImageOnlyTransform\n","from albumentations.augmentations import functional as F\n","import albumentations as A\n","device = torch.device(\"cuda\")\n","import time\n","import random"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["from albumentations.core.transforms_interface import DualTransform\n","from albumentations.augmentations import functional as F\n","\n","from PIL import Image, ImageOps, ImageEnhance\n","from albumentations.core.transforms_interface import ImageOnlyTransform\n","\n","import numpy as np\n","from PIL import Image, ImageOps, ImageEnhance\n","class GridMask(DualTransform):\n","    def __init__(self, num_grid=3, fill_value=0, rotate=0, mode=0, always_apply=False, p=0.5):\n","        super(GridMask, self).__init__(always_apply, p)\n","        if isinstance(num_grid, int):\n","            num_grid = (num_grid, num_grid)\n","        if isinstance(rotate, int):\n","            rotate = (-rotate, rotate)\n","        self.num_grid = num_grid\n","        self.fill_value = fill_value\n","        self.rotate = rotate\n","        self.mode = mode\n","        self.masks = None\n","        self.rand_h_max = []\n","        self.rand_w_max = []\n","\n","    def init_masks(self, height, width):\n","        if self.masks is None:\n","            self.masks = []\n","            n_masks = self.num_grid[1] - self.num_grid[0] + 1\n","            for n, n_g in enumerate(range(self.num_grid[0], self.num_grid[1] + 1, 1)):\n","                grid_h = height / n_g\n","                grid_w = width / n_g\n","                # this_mask = np.ones((int((n_g + 1) * grid_h), int((n_g + 1) * grid_w))).astype(np.uint8)\n","                this_mask = np.zeros((int((n_g + 1) * grid_h), int((n_g + 1) * grid_w))).astype(np.uint8)\n","\n","                for i in range(n_g + 1):\n","                    for j in range(n_g + 1):\n","                        this_mask[\n","                             int(i * grid_h) : int(i * grid_h + grid_h / 2),\n","                             int(j * grid_w) : int(j * grid_w + grid_w / 2)\n","                        ] = self.fill_value\n","                        if self.mode == 2:\n","                            this_mask[\n","                                 int(i * grid_h + grid_h / 2) : int(i * grid_h + grid_h),\n","                                 int(j * grid_w + grid_w / 2) : int(j * grid_w + grid_w)\n","                            ] = self.fill_value\n","                \n","                if self.mode == 1:\n","                    this_mask = 1 - this_mask\n","\n","                self.masks.append(this_mask)\n","                self.rand_h_max.append(grid_h)\n","                self.rand_w_max.append(grid_w)\n","\n","    def apply(self, image, mask, rand_h, rand_w, angle, **params):\n","        h, w = image.shape[:2]\n","        mask = F.rotate(mask, angle) if self.rotate[1] > 0 else mask\n","        mask = mask[:,:,np.newaxis] if image.ndim == 3 else mask\n","        image =np.maximum(image , mask[rand_h:rand_h+h, rand_w:rand_w+w].astype(image.dtype))\n","        # image =image | mask[rand_h:rand_h+h, rand_w:rand_w+w].astype(image.dtype)\n","        return image\n","\n","    def get_params_dependent_on_targets(self, params):\n","        img = params['image']\n","        height, width = img.shape[:2]\n","        self.init_masks(height, width)\n","\n","        mid = np.random.randint(len(self.masks))\n","        mask = self.masks[mid]\n","        rand_h = np.random.randint(self.rand_h_max[mid])\n","        rand_w = np.random.randint(self.rand_w_max[mid])\n","        angle = np.random.randint(self.rotate[0], self.rotate[1]) if self.rotate[1] > 0 else 0\n","\n","        return {'mask': mask, 'rand_h': rand_h, 'rand_w': rand_w, 'angle': angle}\n","\n","    @property\n","    def targets_as_params(self):\n","        return ['image']\n","\n","    def get_transform_init_args_names(self):\n","        return ('num_grid', 'fill_value', 'rotate', 'mode')"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["class Albumentations_cls():\n","    def __init__(self, augmentations):\n","        self.augmentations  = A.Compose(augmentations)\n","    \n","    def __call__(self, image):\n","        return self.augmentations(image = image)[\"image\"]"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["class Dataset(torch.utils.data.Dataset):\n","    def __init__(self, images_path, df, folds, mode, transform = None, transforms_orig= None ):\n","        df = df[df.fold.isin(folds)].reset_index(drop = True)\n","        self.image_ids = df[\"Frame_ID\"].values\n","        self.labels = None\n","        if mode !=\"test\":\n","            self.labels = df[\"Emotion_id\"].values\n","        self.images_path = images_path\n","        self.mode = mode\n","        self.transforms = transform\n","        self.transforms_orig = transforms_orig\n","        \n","    def __len__(self):\n","        return len(self.image_ids)\n","    def __getitem__(self, index):\n","        image = cv2.imread(data_dir + self.images_path + \"/\" + self.image_ids[index])\n","        image_orig = image.astype(np.float32).copy()\n","        \n","        if self.transforms:\n","            image = self.transforms(image)\n","            image = transforms.Normalize(MEAN, STD)(image)\n","        \n","        if self.transforms_orig:\n","            image_orig = self.transforms(image_orig)\n","            image_orig = transforms.Normalize(MEAN, STD)(image_orig)\n","            \n","            \n","        if self.mode != \"test\":\n","            label = self.labels[index]\n","            return torch.tensor(image), torch.tensor(image_orig), torch.tensor(label)\n","        \n","        return torch.tensor(image), torch.tensor(image_orig)\n","            "]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["freeze_layers = 600\n","FREEZE = True\n","if FREEZE:\n","    if freeze_layers == None:\n","        kernel_type = \"efficientnet-b7_base_freeze_gridmask_augmix_fulltrain_extra_dataset\"\n","    else:\n","        kernel_type = f\"efficientnet-b7_{freeze_layers}_freeze_gridmask_augmix_fulltrain_extra_dataset\"\n","else:\n","    kernel_type = \"efficientnet-b7_gridmask_augmix_fulltrain_extra_dataset\"\n","backbone = \"resnet-18\"\n","RANDOM_STATE = 47\n","MEAN = [0.485, 0.456, 0.406] \n","STD = [0.229, 0.224, 0.225]\n","n_folds = 5\n","n_epochs = 380\n","HEIGHT = 224\n","WIDTH = 224\n","num_workers = 0\n","batch_size = 32\n","data_dir = \"./Dataset/\"\n","idx2class = {i:class_name for i,class_name in enumerate([\"angry\", \"happy\", \"sad\", \"surprised\", \"Unknown\"]) }\n","class2idx = {class_name:i for i,class_name in enumerate([\"angry\", \"happy\", \"sad\", \"surprised\", \"Unknown\"]) }\n","out_dim = len([\"angry\", \"happy\", \"sad\", \"surprised\", \"Unknown\"])\n","fold = 6\n","out_dim  = len(idx2class)\n","init_lr = 0.01"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["df = pd.read_csv(data_dir + \"train_corrected_balanced.csv\")\n","df.loc[df.Emotion == \"suprised\", \"Emotion\"] = \"surprised\"\n","df[\"Emotion_id\"] = df.Emotion.map(class2idx)\n","skf = StratifiedKFold(n_folds, shuffle = True, random_state = RANDOM_STATE)\n","df = df[df.Emotion != \"Unknown\"]\n","df = df[df.Frame_ID.isin(os.listdir(data_dir + \"train\"))].reset_index(drop = True)\n","for i_fold, (train_idx, val_idx) in enumerate(skf.split(df, df.Emotion_id)):\n","    df.loc[val_idx, \"fold\"] = i_fold\n","df.fold = df.fold.astype(np.int)\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["preprocess = [\n","    A.Resize(height = HEIGHT, width = WIDTH, always_apply=True),\n","]\n","\n","augmentations = [\n","    A.OneOf([\n","        A.MotionBlur(blur_limit=3),\n","        A.MedianBlur(blur_limit=3),\n","        A.GaussianBlur(blur_limit=3),\n","    ], p=0.65),\n","    # A.OneOf([\n","    #     A.OpticalDistortion(distort_limit=0.5),\n","    #     A.GridDistortion(num_steps=2, distort_limit=0.5),\n","    # ], p=0.6),\n","    A.imgaug.transforms.IAAAffine(shear=5, mode='constant', cval=255, p = 0.65),\n","    A.OneOf([\n","        A.ShiftScaleRotate(rotate_limit=90, border_mode=cv2.BORDER_CONSTANT, value=[255, 255, 255], mask_value=[255, 255, 255], always_apply=True),\n","        GridMask(mode = 0, always_apply= True),\n","        GridMask(mode = 1, always_apply= True),\n","        GridMask(mode = 2, always_apply= True)\n","    ], 0.75)\n","]\n","transforms_train = transforms.Compose([\n","    np.uint8,    \n","    Albumentations_cls(preprocess + augmentations),\n","    transforms.ToTensor(),\n","    \n","])\n","\n","transforms_val = transforms.Compose([\n","    np.uint8,\n","    # transforms.Normalize(mean = MEAN, std = STD),\n","    Albumentations_cls(preprocess),\n","    transforms.ToTensor(),\n","    \n","])\n","transforms_orig = transforms.Compose([\n","    np.uint8,    \n","    Albumentations_cls(preprocess ),\n","    transforms.ToTensor(),\n","])"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["pretrained_dict = {f'efficientnet-b{i}': path for i,path in enumerate(sorted(glob.glob('../input/efficientnet-pytorch/*pth')))}\n","from efficientnet_pytorch import EfficientNet\n","import torchvision.models as models\n","sigmoid = nn.Sigmoid()\n","class Swish(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, i):\n","        result = i * sigmoid(i)\n","        ctx.save_for_backward(i)\n","        return result\n","    \n","    def backward(ctx, grad_output):\n","        i = ctx.saved_variables[0]\n","        sigmoid_i = sigmoid(i)\n","        return grad_output * (sigmoid_i + i*(1 - sigmoid_i))\n","swish = Swish.apply\n","class Swish_module(nn.Module):\n","    def forward(self, x):\n","        return swish(x)\n","    \n","swish_layer = Swish_module()\n","\n","def relu_fn(x):\n","    return swish_layer(x)\n","\n","class GlobalAvgPool(nn.Module):\n","        def __init__(self):\n","            super(GlobalAvgPool, self).__init__()\n","        def forward(self, x):\n","            return x.view(*(x.shape[:-2]),-1).mean(-1)\n","\n","\n","class Seq_Ex_Block(nn.Module):\n","        def __init__(self, in_ch, r):\n","            super(Seq_Ex_Block, self).__init__()\n","            self.se = nn.Sequential(\n","                GlobalAvgPool(),\n","                nn.Linear(in_ch, in_ch//r),\n","                nn.ReLU(inplace=True),\n","                nn.Linear(in_ch//r, in_ch),\n","                nn.Sigmoid()\n","            )\n","\n","        def forward(self, x):\n","            se_weight = self.se(x).unsqueeze(-1).unsqueeze(-1)\n","            #print(f'x:{x.sum()}, x_se:{x.mul(se_weight).sum()}')\n","            return x.mul(se_weight)\n","\n","class Flatten(nn.Module):\n","    def forward(self, input):\n","        return input.view(input.size(0), -1)\n","      \n","class ClassifierNew(nn.Module):\n","    def __init__(self, inp = 2208, h1=1024, out = 102, d=0.35):\n","        super().__init__()\n","        self.ap = nn.AdaptiveAvgPool2d((1,1))\n","        self.mp = nn.AdaptiveMaxPool2d((1,1))\n","        self.fla = Flatten()\n","        self.bn0 = nn.BatchNorm1d(inp*2,eps=1e-05, momentum=0.1, affine=True)\n","        self.dropout0 = nn.Dropout(d)\n","        self.fc1 = nn.Linear(inp*2, h1)\n","        self.bn1 = nn.BatchNorm1d(h1,eps=1e-05, momentum=0.1, affine=True)\n","        self.dropout1 = nn.Dropout(d)\n","        self.fc2 = nn.Linear(h1, out)\n","        self.activation = nn.Softmax()\n","        \n","    def forward(self, x):\n","        ap = self.ap(x)\n","        mp = self.mp(x)\n","        x = torch.cat((ap,mp),dim=1)\n","        x = self.fla(x)\n","        x = self.bn0(x)\n","        x = self.dropout0(x)\n","        x = FT.relu(self.fc1(x))\n","        x = self.bn1(x)\n","        x = self.dropout1(x)         \n","        x = self.fc2(x)\n","        x = self.activation(x)\n","        return x\n","class NeuralNet(nn.Module):\n","    def __init__(self, pretrained = True, Freeze_base = False, layers_freeze = None):\n","        super(NeuralNet, self).__init__()\n","        # self.cnn = models.resnet101(pretrained= pretrained)\n","        # self.cnn = models.resnet50(pretrained= pretrained)\n","        self.cnn = EfficientNet.from_pretrained('efficientnet-b7')\n","        self.cnn = nn.Sequential(*list(self.cnn.children())[:-2])\n","        if Freeze_base:\n","            if layers_freeze == None:\n","                for p in self.cnn.parameters():\n","                    p.requires_grad = False\n","            else:\n","                c = 0\n","                for p in self.cnn.parameters():\n","                    c+=1\n","                    if c < layers_freeze:\n","                        p.requires_grad = False\n","                    else:\n","                        p.requires_grad = True\n","                        \n","\n","        self.fc = ClassifierNew(2048, 1024, 4, 0.35)\n","    def forward(self, input):\n","        x = self.cnn(input)\n","        x = self.fc(x)\n","        return x\n","class EfficientNet_NeuralNet(nn.Module):\n","    def __init__(self, pretrained = True, Freeze_base = False, layers_freeze = None):\n","        super(EfficientNet_NeuralNet, self).__init__()\n","        \n","        self.cnn = EfficientNet.from_pretrained('efficientnet-b7')\n","        self.cnn._avg_pooling = nn.Identity()\n","        self.cnn._dropout = nn.Identity()\n","        self.cnn._swish = nn.Identity()\n","        if Freeze_base:\n","            if layers_freeze == None:\n","                for p in self.cnn.parameters():\n","                    p.requires_grad = False\n","            else:\n","                c = 0\n","                for p in self.cnn.parameters():\n","                    c+=1\n","                    if c < layers_freeze:\n","                        p.requires_grad = False\n","                    else:\n","                        p.requires_grad = True\n","        self.fc = ClassifierNew(2560, 1024, 4, 0.35)\n","        self.cnn._fc = nn.Identity()\n","    def forward(self, input):\n","        x = self.cnn.extract_features(input)\n","        x = self.fc(x)\n","        return x"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["from torch.autograd import Variable\n","\n","class FocalLoss(nn.Module):\n","    def __init__(self, gamma=0, alpha=None, size_average=True):\n","        super(FocalLoss, self).__init__()\n","        self.gamma = gamma\n","        self.alpha = alpha\n","        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])\n","        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n","        self.size_average = size_average\n","\n","    def forward(self, input, target):\n","        if input.dim()>2:\n","            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n","            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n","            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n","        target = target.view(-1,1)\n","\n","        logpt = FT.log_softmax(input)\n","        logpt = logpt.gather(1,target)\n","        logpt = logpt.view(-1)\n","        pt = Variable(logpt.data.exp())\n","\n","        if self.alpha is not None:\n","            if self.alpha.type()!=input.data.type():\n","                self.alpha = self.alpha.type_as(input.data)\n","            at = self.alpha.gather(0,target.data.view(-1))\n","            logpt = logpt * Variable(at)\n","\n","        loss = -1 * (1-pt)**self.gamma * logpt\n","        if self.size_average: return loss.mean()\n","        else: return loss.sum()"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["class F1_Loss(nn.Module):\n","\n","    def __init__(self, epsilon=1e-7):\n","        super().__init__()\n","        self.epsilon = epsilon\n","        \n","    def forward(self, y_pred, y_true,):\n","        assert y_pred.ndim == 2\n","        assert y_true.ndim == 1\n","        y_true_unique = torch.tensor([0,1,2,3])\n","        w = torch.stack([(y_true==x_u).sum() for x_u in y_true_unique])\n","\n","        y_true = FT.one_hot(y_true, 4).to(torch.float32)\n","        y_pred = FT.softmax(y_pred, dim=1)\n","\n","\n","        tp = (y_true * y_pred).sum(dim=0).to(torch.float32)\n","        tn = ((1 - y_true) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n","        fp = ((1 - y_true) * y_pred).sum(dim=0).to(torch.float32)\n","        fn = (y_true * (1 - y_pred)).sum(dim=0).to(torch.float32)\n","\n","        precision = tp / (tp + fp + self.epsilon)\n","        recall = tp / (tp + fn + self.epsilon)\n","\n","        f1 = 2* (precision*recall) / (precision + recall + self.epsilon)\n","        f1 = f1.clamp(min=self.epsilon, max=1-self.epsilon)\n","        return 1 - ((f1 * w).sum(dim = 0))/(w.sum(dim = 0))\n","        # return 1 - f1.mean()\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# logits = torch.from_numpy(np.asarray([[1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0]]))\n","# targets = torch.from_numpy(np.asarray([0,2])).to(torch.int64)\n","# loss = F1_Loss()(logits, targets)\n","# print(loss)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["from sklearn.metrics import f1_score\n","def criterion(logits, targets):\n","    return FocalLoss()(logits, targets)\n","    # return nn.CrossEntropyLoss(weight = cls_wts)(logits, targets)\n","    # return nn.CrossEntropyLoss()(logits, targets)\n","    # return F1_Loss()(logits, targets)\n","\n","\n","\n","def get_score(submission, solution):\n","    y_pred = submission.Emotion_id.values\n","    y_true = solution.Emotion_id.values\n","    return 100 * (f1_score(y_pred, y_true, average = \"weighted\"))"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["from sklearn.metrics import confusion_matrix\n","\n","\n","def train_epoch(loader, optimizer):\n","    model.train()\n","    bar = tqdm(loader)\n","    train_loss = []\n","    \n","    for (data, data_orig, targets) in bar:\n","        optimizer.zero_grad()\n","        data, data_orig, targets = data.to(device), data_orig.to(device), targets.to(device)\n","        \n","        loss_func = criterion\n","\n","        cutmix_threshold = random.uniform(0,1)\n","        \n","        logits = model(data)\n","        \n","        loss = loss_func(logits, targets)\n","        \n","        loss.backward()\n","        optimizer.step()\n","        \n","        loss_np = loss.detach().cpu().numpy()\n","        train_loss.append(loss_np)\n","        smooth_loss = sum(train_loss[-20:])/min(len(train_loss) ,20)\n","        \n","        bar.set_description('loss: %.5f, smth: %.5f' % (loss_np, smooth_loss))\n","        \n","    return train_loss\n","\n","def val_epoch(loader, get_output = False):\n","    model.eval()\n","    val_loss = []\n","    outputs = []\n","    LOGITS = []\n","    acc = 0\n","    pred = []\n","    with torch.no_grad():\n","        for (data, data_orig, target) in tqdm(loader):\n","            data, data_orig, target = data.to(device), data_orig.to(device), target.to(device)\n","            logits = model(data)\n","            loss = criterion(logits, target)\n","            pred = logits.argmax(1).detach()\n","            outputs.append(pred)\n","            acc += (target == pred).sum().detach().cpu().numpy()\n","            if get_output:\n","                LOGITS.append(logits)\n","            val_loss.append(loss.cpu().numpy())\n","        val_loss = np.mean(val_loss)\n","        acc /= len(dataset_valid)\n","    solution = df.iloc[valid_idx]\n","    preds = torch.cat(outputs).cpu().numpy()\n","    score = get_score(pd.DataFrame({\"Emotion_id\": preds}),pd.DataFrame({\"Emotion_id\": solution.Emotion_id.values}) )\n","    if DEBUG:\n","        print()\n","        print(confusion_matrix(solution.Emotion_id.values, preds, ))\n","        print()\n","    if get_output:\n","        LOGITS = torch.cat(LOGITS).detach().cpu().numpy()\n","        return LOGITS\n","    else:\n","        return val_loss, acc, score"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"3    19\n2    19\n1    19\n0    18\nName: Emotion_id, dtype: int64\n3    19\n2    19\n1    19\n0    18\nName: Emotion_id, dtype: int64\n2    19\n1    19\n0    19\n3    18\nName: Emotion_id, dtype: int64\n3    19\n0    19\n2    18\n1    18\nName: Emotion_id, dtype: int64\n3    19\n2    19\n1    18\n0    18\nName: Emotion_id, dtype: int64\n"}],"source":["for i in range(n_folds):\n","    print(df[df.fold == i].Emotion_id.value_counts())"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Loaded pretrained weights for efficientnet-b7\n"}],"source":["DEBUG = True\n","\n","record = [{'train_loss': [], 'val_loss': [], \"score\": []} for x in range(n_folds)]\n","\n","\n","i_fold = fold\n","folds = [i for i in range(n_folds)]\n","train_idx, valid_idx = np.where((df['fold'] != i_fold))[0], np.where((df['fold'] == i_fold))[0]\n","\n","train_folds = []\n","val_folds = []\n","for i in range(n_folds):\n","    if i == fold:\n","        val_folds.append(i)\n","    else:\n","        train_folds.append(i)\n","\n","dataset_train = Dataset(\"train\", df, train_folds,\"train\",transforms_train, transforms_orig )\n","dataset_valid = Dataset(\"train\", df, val_folds,\"validation\",transforms_val, transforms_orig )\n","\n","\n","train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True,num_workers=num_workers)\n","valid_loader = torch.utils.data.DataLoader(dataset_valid, batch_size=batch_size, shuffle=False, sampler=None, num_workers=num_workers)\n","\n","model = EfficientNet_NeuralNet(pretrained= True, Freeze_base= FREEZE, layers_freeze = freeze_layers)\n","model = model.to(device)\n","\n","max_score = 0\n","model_file = f'{kernel_type}_best_fold{i_fold}.pth'\n","# model.load_state_dict(torch.load(\"./resnet-101_base_freeze_gridmask_augmix_fulltrain_best_fold0_epoch60.pth\"))\n","\n","# optimizer = torch.optim.SGD(model.parameters(), lr = init_lr, momentum = 0.9, )\n","# optimizer = torch.optim.Adadelta(model.parameters(), lr = init_lr )\n","optimizer = torch.optim.Adam(model.parameters(), lr = init_lr)\n","\n","\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"2/12 [00:12<00:00,  1.07s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:00:29 2020 Epoch 291, lr: 0.0062941, train loss: 0.86568\nFri Apr 24 22:00:29 2020 Epoch: 292\nloss: 0.93414, smth: 0.87541: 100%|██████████| 12/12 [00:12<00:00,  1.06s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:00:42 2020 Epoch 292, lr: 0.0061672, train loss: 0.87541\nFri Apr 24 22:00:42 2020 Epoch: 293\nloss: 0.83891, smth: 0.88185: 100%|██████████| 12/12 [00:12<00:00,  1.06s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:00:55 2020 Epoch 293, lr: 0.0060396, train loss: 0.88185\nFri Apr 24 22:00:55 2020 Epoch: 294\nloss: 0.93414, smth: 0.90651: 100%|██████████| 12/12 [00:13<00:00,  1.09s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:01:08 2020 Epoch 294, lr: 0.0059112, train loss: 0.90651\nFri Apr 24 22:01:08 2020 Epoch: 295\nloss: 0.88653, smth: 0.85294: 100%|██████████| 12/12 [00:12<00:00,  1.06s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:01:21 2020 Epoch 295, lr: 0.0057822, train loss: 0.85294\nFri Apr 24 22:01:21 2020 Epoch: 296\nloss: 0.83891, smth: 0.87802: 100%|██████████| 12/12 [00:12<00:00,  1.07s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:01:33 2020 Epoch 296, lr: 0.0056526, train loss: 0.87802\nFri Apr 24 22:01:33 2020 Epoch: 297\nloss: 0.88653, smth: 0.88540: 100%|██████████| 12/12 [00:12<00:00,  1.07s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:01:46 2020 Epoch 297, lr: 0.0055226, train loss: 0.88540\nFri Apr 24 22:01:46 2020 Epoch: 298\nloss: 1.07952, smth: 0.89153: 100%|██████████| 12/12 [00:12<00:00,  1.08s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:01:59 2020 Epoch 298, lr: 0.0053923, train loss: 0.89153\nFri Apr 24 22:01:59 2020 Epoch: 299\nloss: 0.88653, smth: 0.92341: 100%|██████████| 12/12 [00:12<00:00,  1.07s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:02:12 2020 Epoch 299, lr: 0.0052617, train loss: 0.92341\nFri Apr 24 22:02:12 2020 Epoch: 300\nloss: 0.88653, smth: 0.89390: 100%|██████████| 12/12 [00:12<00:00,  1.06s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:02:25 2020 Epoch 300, lr: 0.0051309, train loss: 0.89390\nFri Apr 24 22:02:25 2020 Epoch: 301\nloss: 0.83891, smth: 0.87370: 100%|██████████| 12/12 [00:12<00:00,  1.07s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:02:38 2020 Epoch 301, lr: 0.0050000, train loss: 0.87370\nFri Apr 24 22:02:38 2020 Epoch: 302\nloss: 0.88653, smth: 0.85935: 100%|██████████| 12/12 [00:12<00:00,  1.06s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:02:50 2020 Epoch 302, lr: 0.0048691, train loss: 0.85935\nFri Apr 24 22:02:50 2020 Epoch: 303\nloss: 0.74367, smth: 0.85814: 100%|██████████| 12/12 [00:12<00:00,  1.07s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:03:03 2020 Epoch 303, lr: 0.0047383, train loss: 0.85814\nFri Apr 24 22:03:03 2020 Epoch: 304\nloss: 0.93414, smth: 0.86370: 100%|██████████| 12/12 [00:12<00:00,  1.08s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:03:16 2020 Epoch 304, lr: 0.0046077, train loss: 0.86370\nFri Apr 24 22:03:16 2020 Epoch: 305\nloss: 0.79130, smth: 0.84484: 100%|██████████| 12/12 [00:12<00:00,  1.07s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:03:29 2020 Epoch 305, lr: 0.0044774, train loss: 0.84484\nFri Apr 24 22:03:29 2020 Epoch: 306\nloss: 0.93402, smth: 0.86807: 100%|██████████| 12/12 [00:12<00:00,  1.07s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:03:42 2020 Epoch 306, lr: 0.0043474, train loss: 0.86807\nFri Apr 24 22:03:42 2020 Epoch: 307\nloss: 0.88652, smth: 0.87550: 100%|██████████| 12/12 [00:12<00:00,  1.06s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:03:54 2020 Epoch 307, lr: 0.0042178, train loss: 0.87550\nFri Apr 24 22:03:54 2020 Epoch: 308\nloss: 0.74367, smth: 0.87043: 100%|██████████| 12/12 [00:12<00:00,  1.05s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:04:07 2020 Epoch 308, lr: 0.0040888, train loss: 0.87043\nFri Apr 24 22:04:07 2020 Epoch: 309\nloss: 0.88653, smth: 0.85673: 100%|██████████| 12/12 [00:12<00:00,  1.06s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:04:20 2020 Epoch 309, lr: 0.0039604, train loss: 0.85673\nFri Apr 24 22:04:20 2020 Epoch: 310\nloss: 0.93414, smth: 0.85330: 100%|██████████| 12/12 [00:12<00:00,  1.05s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:04:32 2020 Epoch 310, lr: 0.0038328, train loss: 0.85330\nFri Apr 24 22:04:32 2020 Epoch: 311\nloss: 0.79774, smth: 0.82851: 100%|██████████| 12/12 [00:12<00:00,  1.06s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:04:45 2020 Epoch 311, lr: 0.0037059, train loss: 0.82851\nFri Apr 24 22:04:45 2020 Epoch: 312\nloss: 0.79129, smth: 0.80977: 100%|██████████| 12/12 [00:12<00:00,  1.07s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:04:58 2020 Epoch 312, lr: 0.0035799, train loss: 0.80977\nFri Apr 24 22:04:58 2020 Epoch: 313\nloss: 0.79129, smth: 0.83882: 100%|██████████| 12/12 [00:12<00:00,  1.07s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:05:11 2020 Epoch 313, lr: 0.0034549, train loss: 0.83882\nFri Apr 24 22:05:11 2020 Epoch: 314\nloss: 0.93414, smth: 0.86898: 100%|██████████| 12/12 [00:12<00:00,  1.06s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:05:24 2020 Epoch 314, lr: 0.0033310, train loss: 0.86898\nFri Apr 24 22:05:24 2020 Epoch: 315\nloss: 0.93413, smth: 0.83729: 100%|██████████| 12/12 [00:12<00:00,  1.08s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:05:36 2020 Epoch 315, lr: 0.0032082, train loss: 0.83729\nFri Apr 24 22:05:36 2020 Epoch: 316\nloss: 0.88652, smth: 0.85761: 100%|██████████| 12/12 [00:12<00:00,  1.06s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:05:49 2020 Epoch 316, lr: 0.0030866, train loss: 0.85761\nFri Apr 24 22:05:49 2020 Epoch: 317\nloss: 0.79129, smth: 0.84399: 100%|██████████| 12/12 [00:12<00:00,  1.05s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:06:02 2020 Epoch 317, lr: 0.0029663, train loss: 0.84399\nFri Apr 24 22:06:02 2020 Epoch: 318\nloss: 0.74368, smth: 0.82443: 100%|██████████| 12/12 [00:12<00:00,  1.07s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:06:15 2020 Epoch 318, lr: 0.0028474, train loss: 0.82443\nFri Apr 24 22:06:15 2020 Epoch: 319\nloss: 0.88653, smth: 0.83394: 100%|██████████| 12/12 [00:12<00:00,  1.07s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:06:27 2020 Epoch 319, lr: 0.0027300, train loss: 0.83394\nFri Apr 24 22:06:27 2020 Epoch: 320\nloss: 0.93414, smth: 0.85596: 100%|██████████| 12/12 [00:12<00:00,  1.06s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:06:40 2020 Epoch 320, lr: 0.0026142, train loss: 0.85596\nFri Apr 24 22:06:40 2020 Epoch: 321\nloss: 0.83891, smth: 0.82463: 100%|██████████| 12/12 [00:12<00:00,  1.06s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:06:53 2020 Epoch 321, lr: 0.0025000, train loss: 0.82463\nFri Apr 24 22:06:53 2020 Epoch: 322\nloss: 0.79273, smth: 0.82854: 100%|██████████| 12/12 [00:13<00:00,  1.09s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:07:06 2020 Epoch 322, lr: 0.0023875, train loss: 0.82854\nFri Apr 24 22:07:06 2020 Epoch: 323\nloss: 0.88653, smth: 0.85407: 100%|██████████| 12/12 [00:12<00:00,  1.06s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:07:19 2020 Epoch 323, lr: 0.0022768, train loss: 0.85407\nFri Apr 24 22:07:19 2020 Epoch: 324\nloss: 0.88653, smth: 0.82584: 100%|██████████| 12/12 [00:12<00:00,  1.05s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:07:31 2020 Epoch 324, lr: 0.0021680, train loss: 0.82584\nFri Apr 24 22:07:31 2020 Epoch: 325\nloss: 0.79129, smth: 0.84478: 100%|██████████| 12/12 [00:13<00:00,  1.08s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:07:44 2020 Epoch 325, lr: 0.0020611, train loss: 0.84478\nFri Apr 24 22:07:44 2020 Epoch: 326\nloss: 0.88653, smth: 0.84411: 100%|██████████| 12/12 [00:12<00:00,  1.06s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:07:57 2020 Epoch 326, lr: 0.0019562, train loss: 0.84411\nFri Apr 24 22:07:57 2020 Epoch: 327\nloss: 0.79129, smth: 0.83337: 100%|██████████| 12/12 [00:12<00:00,  1.06s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:08:10 2020 Epoch 327, lr: 0.0018534, train loss: 0.83337\nFri Apr 24 22:08:10 2020 Epoch: 328\nloss: 1.12462, smth: 0.87172: 100%|██████████| 12/12 [00:13<00:00,  1.09s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:08:23 2020 Epoch 328, lr: 0.0017528, train loss: 0.87172\nFri Apr 24 22:08:23 2020 Epoch: 329\nloss: 0.88653, smth: 0.81690: 100%|██████████| 12/12 [00:12<00:00,  1.07s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:08:36 2020 Epoch 329, lr: 0.0016543, train loss: 0.81690\nFri Apr 24 22:08:36 2020 Epoch: 330\nloss: 0.93412, smth: 0.86809: 100%|██████████| 12/12 [00:12<00:00,  1.06s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:08:48 2020 Epoch 330, lr: 0.0015582, train loss: 0.86809\nFri Apr 24 22:08:48 2020 Epoch: 331\nloss: 0.88648, smth: 0.83890: 100%|██████████| 12/12 [00:12<00:00,  1.08s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:09:01 2020 Epoch 331, lr: 0.0014645, train loss: 0.83890\nFri Apr 24 22:09:01 2020 Epoch: 332\nloss: 0.93405, smth: 0.87478: 100%|██████████| 12/12 [00:12<00:00,  1.07s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:09:14 2020 Epoch 332, lr: 0.0013731, train loss: 0.87478\nFri Apr 24 22:09:14 2020 Epoch: 333\nloss: 0.79129, smth: 0.83288: 100%|██████████| 12/12 [00:12<00:00,  1.05s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:09:27 2020 Epoch 333, lr: 0.0012843, train loss: 0.83288\nFri Apr 24 22:09:27 2020 Epoch: 334\nloss: 0.83891, smth: 0.82454: 100%|██████████| 12/12 [00:12<00:00,  1.07s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:09:40 2020 Epoch 334, lr: 0.0011980, train loss: 0.82454\nFri Apr 24 22:09:40 2020 Epoch: 335\nloss: 0.74367, smth: 0.80330: 100%|██████████| 12/12 [00:12<00:00,  1.08s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:09:53 2020 Epoch 335, lr: 0.0011143, train loss: 0.80330\nFri Apr 24 22:09:53 2020 Epoch: 336\nloss: 0.83891, smth: 0.84526: 100%|██████████| 12/12 [00:12<00:00,  1.06s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:10:05 2020 Epoch 336, lr: 0.0010332, train loss: 0.84526\nFri Apr 24 22:10:05 2020 Epoch: 337\nloss: 0.88653, smth: 0.82075: 100%|██████████| 12/12 [00:12<00:00,  1.08s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:10:18 2020 Epoch 337, lr: 0.0009549, train loss: 0.82075\nFri Apr 24 22:10:18 2020 Epoch: 338\nloss: 0.83891, smth: 0.80235: 100%|██████████| 12/12 [00:12<00:00,  1.06s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:10:31 2020 Epoch 338, lr: 0.0008794, train loss: 0.80235\nFri Apr 24 22:10:31 2020 Epoch: 339\nloss: 0.74367, smth: 0.84360: 100%|██████████| 12/12 [00:12<00:00,  1.07s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:10:44 2020 Epoch 339, lr: 0.0008066, train loss: 0.84360\nFri Apr 24 22:10:44 2020 Epoch: 340\nloss: 0.79129, smth: 0.82263: 100%|██████████| 12/12 [00:12<00:00,  1.05s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:10:56 2020 Epoch 340, lr: 0.0007368, train loss: 0.82263\nFri Apr 24 22:10:56 2020 Epoch: 341\nloss: 0.74367, smth: 0.79670: 100%|██████████| 12/12 [00:12<00:00,  1.08s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:11:09 2020 Epoch 341, lr: 0.0006699, train loss: 0.79670\nFri Apr 24 22:11:09 2020 Epoch: 342\nloss: 0.79129, smth: 0.83356: 100%|██████████| 12/12 [00:12<00:00,  1.06s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:11:22 2020 Epoch 342, lr: 0.0006059, train loss: 0.83356\nFri Apr 24 22:11:22 2020 Epoch: 343\nloss: 0.79133, smth: 0.82573: 100%|██████████| 12/12 [00:12<00:00,  1.06s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:11:35 2020 Epoch 343, lr: 0.0005450, train loss: 0.82573\nFri Apr 24 22:11:35 2020 Epoch: 344\nloss: 0.79129, smth: 0.83618: 100%|██████████| 12/12 [00:12<00:00,  1.07s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:11:48 2020 Epoch 344, lr: 0.0004871, train loss: 0.83618\nFri Apr 24 22:11:48 2020 Epoch: 345\nloss: 0.98176, smth: 0.85619: 100%|██████████| 12/12 [00:12<00:00,  1.06s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:12:01 2020 Epoch 345, lr: 0.0004323, train loss: 0.85619\nFri Apr 24 22:12:01 2020 Epoch: 346\nloss: 0.83891, smth: 0.83474: 100%|██████████| 12/12 [00:12<00:00,  1.07s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:12:13 2020 Epoch 346, lr: 0.0003806, train loss: 0.83474\nFri Apr 24 22:12:13 2020 Epoch: 347\nloss: 0.79129, smth: 0.84663: 100%|██████████| 12/12 [00:12<00:00,  1.08s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:12:26 2020 Epoch 347, lr: 0.0003321, train loss: 0.84663\nFri Apr 24 22:12:26 2020 Epoch: 348\nloss: 0.83891, smth: 0.81680: 100%|██████████| 12/12 [00:12<00:00,  1.07s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:12:39 2020 Epoch 348, lr: 0.0002868, train loss: 0.81680\nFri Apr 24 22:12:39 2020 Epoch: 349\nloss: 0.83891, smth: 0.84532: 100%|██████████| 12/12 [00:12<00:00,  1.08s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:12:52 2020 Epoch 349, lr: 0.0002447, train loss: 0.84532\nFri Apr 24 22:12:52 2020 Epoch: 350\nloss: 0.83891, smth: 0.83683: 100%|██████████| 12/12 [00:12<00:00,  1.07s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:13:05 2020 Epoch 350, lr: 0.0002059, train loss: 0.83683\nFri Apr 24 22:13:05 2020 Epoch: 351\nloss: 0.74367, smth: 0.82207: 100%|██████████| 12/12 [00:12<00:00,  1.06s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:13:18 2020 Epoch 351, lr: 0.0001704, train loss: 0.82207\nFri Apr 24 22:13:18 2020 Epoch: 352\nloss: 0.83891, smth: 0.81872: 100%|██████████| 12/12 [00:12<00:00,  1.07s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:13:31 2020 Epoch 352, lr: 0.0001382, train loss: 0.81872\nFri Apr 24 22:13:31 2020 Epoch: 353\nloss: 0.79129, smth: 0.81043: 100%|██████████| 12/12 [00:12<00:00,  1.06s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:13:43 2020 Epoch 353, lr: 0.0001093, train loss: 0.81043\nFri Apr 24 22:13:43 2020 Epoch: 354\nloss: 0.79129, smth: 0.82050: 100%|██████████| 12/12 [00:12<00:00,  1.07s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:13:56 2020 Epoch 354, lr: 0.0000837, train loss: 0.82050\nFri Apr 24 22:13:56 2020 Epoch: 355\nloss: 0.83891, smth: 0.81900: 100%|██████████| 12/12 [00:12<00:00,  1.08s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:14:09 2020 Epoch 355, lr: 0.0000616, train loss: 0.81900\nFri Apr 24 22:14:09 2020 Epoch: 356\nloss: 0.79129, smth: 0.85437: 100%|██████████| 12/12 [00:12<00:00,  1.08s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:14:22 2020 Epoch 356, lr: 0.0000428, train loss: 0.85437\nFri Apr 24 22:14:22 2020 Epoch: 357\nloss: 0.93414, smth: 0.82597: 100%|██████████| 12/12 [00:12<00:00,  1.07s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:14:35 2020 Epoch 357, lr: 0.0000274, train loss: 0.82597\nFri Apr 24 22:14:35 2020 Epoch: 358\nloss: 0.98176, smth: 0.84581: 100%|██████████| 12/12 [00:12<00:00,  1.06s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:14:48 2020 Epoch 358, lr: 0.0000154, train loss: 0.84581\nFri Apr 24 22:14:48 2020 Epoch: 359\nloss: 0.88653, smth: 0.81775: 100%|██████████| 12/12 [00:12<00:00,  1.07s/it]\nFri Apr 24 22:15:01 2020 Epoch 359, lr: 0.0000069, train loss: 0.81775\nSaving model on epoch 360\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:15:01 2020 Epoch: 360\nloss: 0.83891, smth: 0.82872: 100%|██████████| 12/12 [00:14<00:00,  1.17s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:15:15 2020 Epoch 360, lr: 0.0000017, train loss: 0.82872\nFri Apr 24 22:15:15 2020 Epoch: 361\nloss: 0.93177, smth: 0.82957: 100%|██████████| 12/12 [00:12<00:00,  1.07s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:15:28 2020 Epoch 361, lr: 0.0000000, train loss: 0.82957\nFri Apr 24 22:15:28 2020 Epoch: 362\nloss: 0.88653, smth: 0.83075: 100%|██████████| 12/12 [00:13<00:00,  1.14s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:15:42 2020 Epoch 362, lr: 0.0000017, train loss: 0.83075\nFri Apr 24 22:15:42 2020 Epoch: 363\nloss: 0.88653, smth: 0.83122: 100%|██████████| 12/12 [00:13<00:00,  1.13s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:15:55 2020 Epoch 363, lr: 0.0000069, train loss: 0.83122\nFri Apr 24 22:15:55 2020 Epoch: 364\nloss: 0.93414, smth: 0.84204: 100%|██████████| 12/12 [00:12<00:00,  1.06s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:16:08 2020 Epoch 364, lr: 0.0000154, train loss: 0.84204\nFri Apr 24 22:16:08 2020 Epoch: 365\nloss: 0.83891, smth: 0.80642: 100%|██████████| 12/12 [00:13<00:00,  1.09s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:16:21 2020 Epoch 365, lr: 0.0000274, train loss: 0.80642\nFri Apr 24 22:16:21 2020 Epoch: 366\nloss: 0.88653, smth: 0.82108: 100%|██████████| 12/12 [00:12<00:00,  1.08s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:16:34 2020 Epoch 366, lr: 0.0000428, train loss: 0.82108\nFri Apr 24 22:16:34 2020 Epoch: 367\nloss: 0.79228, smth: 0.81103: 100%|██████████| 12/12 [00:13<00:00,  1.10s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:16:47 2020 Epoch 367, lr: 0.0000616, train loss: 0.81103\nFri Apr 24 22:16:47 2020 Epoch: 368\nloss: 0.88653, smth: 0.79989: 100%|██████████| 12/12 [00:13<00:00,  1.09s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:17:01 2020 Epoch 368, lr: 0.0000837, train loss: 0.79989\nFri Apr 24 22:17:01 2020 Epoch: 369\nloss: 0.79129, smth: 0.80937: 100%|██████████| 12/12 [00:12<00:00,  1.08s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:17:14 2020 Epoch 369, lr: 0.0001093, train loss: 0.80937\nFri Apr 24 22:17:14 2020 Epoch: 370\nloss: 0.98176, smth: 0.82337: 100%|██████████| 12/12 [00:13<00:00,  1.10s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:17:27 2020 Epoch 370, lr: 0.0001382, train loss: 0.82337\nFri Apr 24 22:17:27 2020 Epoch: 371\nloss: 0.83891, smth: 0.84796: 100%|██████████| 12/12 [00:12<00:00,  1.07s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:17:40 2020 Epoch 371, lr: 0.0001704, train loss: 0.84796\nFri Apr 24 22:17:40 2020 Epoch: 372\nloss: 0.79129, smth: 0.82316: 100%|██████████| 12/12 [00:13<00:00,  1.10s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:17:53 2020 Epoch 372, lr: 0.0002059, train loss: 0.82316\nFri Apr 24 22:17:53 2020 Epoch: 373\nloss: 0.88653, smth: 0.83386: 100%|██████████| 12/12 [00:13<00:00,  1.09s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:18:06 2020 Epoch 373, lr: 0.0002447, train loss: 0.83386\nFri Apr 24 22:18:06 2020 Epoch: 374\nloss: 0.74367, smth: 0.83727: 100%|██████████| 12/12 [00:12<00:00,  1.08s/it]\nFri Apr 24 22:18:19 2020 Epoch 374, lr: 0.0002868, train loss: 0.83727\nSaving model on epoch 375\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:18:19 2020 Epoch: 375\nloss: 0.79129, smth: 0.82522: 100%|██████████| 12/12 [00:13<00:00,  1.10s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:18:32 2020 Epoch 375, lr: 0.0003321, train loss: 0.82522\nFri Apr 24 22:18:32 2020 Epoch: 376\nloss: 0.83891, smth: 0.80657: 100%|██████████| 12/12 [00:13<00:00,  1.09s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:18:45 2020 Epoch 376, lr: 0.0003806, train loss: 0.80657\nFri Apr 24 22:18:45 2020 Epoch: 377\nloss: 0.88653, smth: 0.81791: 100%|██████████| 12/12 [00:13<00:00,  1.10s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:18:59 2020 Epoch 377, lr: 0.0004323, train loss: 0.81791\nFri Apr 24 22:18:59 2020 Epoch: 378\nloss: 0.83891, smth: 0.82877: 100%|██████████| 12/12 [00:12<00:00,  1.08s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:19:12 2020 Epoch 378, lr: 0.0004871, train loss: 0.82877\nFri Apr 24 22:19:12 2020 Epoch: 379\nloss: 0.74367, smth: 0.82664: 100%|██████████| 12/12 [00:13<00:00,  1.09s/it]\n  0%|          | 0/12 [00:00<?, ?it/s]Fri Apr 24 22:19:25 2020 Epoch 379, lr: 0.0005450, train loss: 0.82664\nFri Apr 24 22:19:25 2020 Epoch: 380\nloss: 0.93414, smth: 0.83089: 100%|██████████| 12/12 [00:13<00:00,  1.11s/it]\nFri Apr 24 22:19:38 2020 Epoch 380, lr: 0.0006059, train loss: 0.83089\n"}],"source":["scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=False, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08)\n","\n","lmbda = lambda epoch: 0.95\n","scheduler_mul = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)\n","scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 120,  last_epoch= -1)\n","print('Training All Layers...\\n')\n","\n","for epoch in range(1, n_epochs+1):\n","    print(time.ctime(), 'Epoch:', epoch)\n","    \n","    scheduler_cosine.step(epoch-1)\n","    train_loss = train_epoch(train_loader, optimizer)\n","    \n","    # val_loss, acc, score = val_epoch(valid_loader)\n","    \n","    # scheduler.step(val_loss)\n","    # content = time.ctime() + ' ' + f'Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {np.mean(train_loss):.5f}, val loss: {np.mean(val_loss):.5f}, acc: {(acc):.5f}, score: {(score):.6f}'\n","    \n","    \n","    content = time.ctime() + ' ' + f'Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {np.mean(train_loss):.5f}'\n","    \n","    scheduler_mul.step()\n","\n","    print(content)\n","    with open(f'log_{kernel_type}.txt', 'a') as appender:\n","        appender.write(content + '\\n')\n","        \n","    if (epoch+1) % 125==0 or (epoch+1)%120 == 0:\n","        print(f\"Saving model on epoch {epoch+1}\")\n","        torch.save(model.state_dict(), f'{kernel_type}_best_fold{i_fold}_epoch{epoch+1}.pth')\n","\n","    # if score > max_score:\n","    #     print('score ({:.6f} --> {:.6f}).  Saving model ...'.format(max_score, score))\n","    #     torch.save(model.state_dict(), f'{kernel_type}_best_fold{i_fold}.pth')\n","    #     max_score = score\n","        \n","    # record[i_fold]['train_loss'].append(np.mean(train_loss))\n","    # record[i_fold]['val_loss'].append(val_loss)\n","    # record[i_fold]['score'].append(score)\n","    \n","torch.save(model.state_dict(), os.path.join(f'{kernel_type}_model_fold{i_fold}.pth'))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.7.4-final","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}